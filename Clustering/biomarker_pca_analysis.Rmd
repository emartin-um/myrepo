---
title: "Biomarker PCA Analysis"
author: "Analysis Report"
date: "`r Sys.Date()`"
output:
  html_document:
    toc: true
    toc_float: true
    code_folding: hide
    theme: flatly
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE, fig.width = 10, fig.height = 8)
```

# Set-up

## Load Libraries

```{r load-libraries}
library(tidyverse)
library(ggplot2)
library(factoextra)
library(FactoMineR)
library(gridExtra)
library(scales)
```

## Load Data

```{r load-data}
# Read the data
data <- read.csv("input_files/meta_plus_race_dx.csv", stringsAsFactors = FALSE)

# Display data dimensions
cat("Data dimensions:", dim(data)[1], "samples x", dim(data)[2], "columns\n")

# Display first few rows and columns
head(data[, 1:10])
```

## Create AB42/AB40 Ratio Biomarker

```{r create-ab-ratio}
# Function to create AB42/AB40 ratio biomarker
# Since values are in log2 scale, AB42 - AB40 = log2(AB42/AB40)
create_ab42_ab40_ratio <- function(input_file = "input_files/meta_plus_race_dx.csv") {

  # Read the data
  data <- read.csv(input_file, stringsAsFactors = FALSE)

  # Check if ratio column already exists
  if ("AB42_AB40_ratio" %in% colnames(data)) {
    cat("AB42_AB40_ratio column already exists in the dataset.\n")
    return(data)
  }

  # Check if AB42 and AB40 columns exist
  if (!("Aβ42" %in% colnames(data)) || !("Aβ40" %in% colnames(data))) {
    cat("ERROR: Required columns Aβ42 and/or Aβ40 not found in the dataset.\n")
    cat("Available columns:", paste(colnames(data), collapse = ", "), "\n")
    return(data)
  }

  # Calculate the ratio (log2 scale: difference = ratio)
  data$AB42_AB40_ratio <- data$`Aβ42` - data$`Aβ40`

  cat("Successfully created AB42_AB40_ratio biomarker.\n")
  cat("This represents log2(AB42/AB40) since the original values are in log2 scale.\n")
  cat("Formula: AB42_AB40_ratio = Aβ42 - Aβ40\n\n")

  # Show summary statistics
  cat("Summary statistics for new biomarker:\n")
  print(summary(data$AB42_AB40_ratio))
  cat("\n")

  # Save updated data back to file
  write.csv(data, input_file, row.names = FALSE)
  cat("Updated data saved to:", input_file, "\n")
  cat("New dimensions:", dim(data)[1], "samples x", dim(data)[2], "columns\n\n")

  return(data)
}

# Create the AB42/AB40 ratio and update the input file
data <- create_ab42_ab40_ratio("input_files/meta_plus_race_dx.csv")
```

## Configuration - Edit This Section

```{r configuration}
# ==============================================================================
# CONFIGURATION - EDIT THIS SECTION TO CUSTOMIZE YOUR ANALYSIS
# ==============================================================================

# --- DATA SUBSETTING ---
# Set SUBSET_DATA to TRUE to enable subsetting, FALSE to use all data
SUBSET_DATA <- FALSE

# Choose which variable to subset by (e.g., "CDX", "Race_Ethnicity", "sex", "Site", etc.)
SUBSET_VARIABLE <- "CDX"

# Choose which values to include (use a vector for multiple values)
# Examples:
#   For CDX: c("AD", "MCI") - only include AD and MCI samples
#   For Race_Ethnicity: c("WH_HI", "MU_HI") - only include these groups
#   For sex: c("F") - only include females
SUBSET_VALUES <- c("NCI")

# --- ASSOCIATION TESTING ---
# Minimum group size to include in analysis
MIN_GROUP_SIZE <- 5

# Number of top PCs to test
N_PCS_TO_TEST <- 15

# Metadata variables to test for associations with PCs
VARS_TO_TEST <- c("CDX", "Race_Ethnicity", "sex", "APOE.geno", "Site",
                  "Run", "Bay", "age_at_sample", "Group")

# --- PLOTTING OPTIONS ---
# Significance threshold for plotting (adjusted p-value)
PLOT_P_THRESHOLD <- 0.05

# Number of top biomarkers to show for each PC in loadings plots
N_TOP_LOADINGS <- 15

# Number of PCs to analyze for biomarker loadings
N_PCS_LOADINGS <- 10

# ==============================================================================
```

## Data Preparation

```{r data-prep}
# Define metadata columns (first few columns and last 2 in original file)
# Based on the file structure: first 23 columns are metadata,
# columns 24-126 are biomarkers (APOE through pTau-217),
# columns 127-128 are Race_Ethnicity and Run_Bay (metadata),
# column 129+ are any new biomarkers added (e.g., AB42_AB40_ratio)

# Fixed metadata columns
metadata_cols_fixed <- c(1:23, 127:128)

# Biomarker columns: start at 24 and go until we hit Race_Ethnicity (column 127)
# Then add any columns after Run_Bay (column 128) which would be new biomarkers
biomarker_cols_range1 <- 24:126  # Original biomarkers
if (ncol(data) > 128) {
  # Include any new biomarker columns added after Run_Bay
  biomarker_cols_range2 <- 129:ncol(data)
  biomarker_cols <- c(biomarker_cols_range1, biomarker_cols_range2)
  cat("Including", length(biomarker_cols_range2), "additional biomarker column(s):",
      paste(colnames(data)[biomarker_cols_range2], collapse = ", "), "\n")
} else {
  biomarker_cols <- biomarker_cols_range1
}

# Extract metadata and biomarker data
metadata <- data[, metadata_cols_fixed]
biomarkers <- data[, biomarker_cols]

cat("Total biomarkers for analysis:", ncol(biomarkers), "\n")

# Apply subsetting if enabled
if (SUBSET_DATA) {
  if (SUBSET_VARIABLE %in% colnames(metadata)) {
    subset_idx <- metadata[[SUBSET_VARIABLE]] %in% SUBSET_VALUES
    cat("SUBSETTING DATA:\n")
    cat("Variable:", SUBSET_VARIABLE, "\n")
    cat("Values included:", paste(SUBSET_VALUES, collapse = ", "), "\n")
    cat("Samples before subsetting:", nrow(metadata), "\n")
    cat("Samples after subsetting:", sum(subset_idx), "\n\n")

    # Show distribution
    cat("Sample distribution:\n")
    print(table(metadata[[SUBSET_VARIABLE]][subset_idx]))
    cat("\n")

    metadata <- metadata[subset_idx, ]
    biomarkers <- biomarkers[subset_idx, ]
  } else {
    cat("WARNING: Subset variable '", SUBSET_VARIABLE, "' not found in metadata.\n")
    cat("Available variables:", paste(colnames(metadata), collapse = ", "), "\n")
    cat("Proceeding with full dataset.\n\n")
  }
} else {
  cat("Using full dataset (no subsetting applied)\n\n")
}

# Convert biomarkers to numeric matrix
biomarkers_numeric <- as.matrix(biomarkers)
biomarkers_numeric <- apply(biomarkers_numeric, 2, as.numeric)

# Remove samples with missing values in biomarkers
complete_samples <- complete.cases(biomarkers_numeric)
cat("Complete samples:", sum(complete_samples), "out of", nrow(biomarkers_numeric), "\n")
cat("Samples removed due to missing values:", sum(!complete_samples), "\n")

biomarkers_complete <- biomarkers_numeric[complete_samples, ]
metadata_complete <- metadata[complete_samples, ]

# Store biomarker names
biomarker_names <- colnames(biomarkers)
cat("\nNumber of biomarkers:", length(biomarker_names), "\n")
cat("\nBiomarkers included (first 10):\n")
print(head(biomarker_names, 10))
cat("...\n")
cat("Last 5 biomarkers:\n")
print(tail(biomarker_names, 5))
```

## Perform PCA

```{r perform-pca}
# Perform PCA on scaled biomarker data
pca_result <- prcomp(biomarkers_complete, scale. = TRUE, center = TRUE)

# Summary of PCA
summary_pca <- summary(pca_result)
cat("PCA Summary:\n")
print(summary_pca$importance[, 1:10])

# Create a data frame with PC scores and metadata
pca_scores <- as.data.frame(pca_result$x)
pca_data <- cbind(pca_scores, metadata_complete)
```

# Plots

## Scree Plot

```{r scree-plot, fig.width=12, fig.height=6}
# Calculate variance explained
variance_explained <- summary_pca$importance[2, ] * 100
cumulative_variance <- summary_pca$importance[3, ] * 100

# Create scree plot data
scree_data <- data.frame(
  PC = 1:min(20, ncol(pca_result$x)),
  Variance = variance_explained[1:min(20, ncol(pca_result$x))],
  Cumulative = cumulative_variance[1:min(20, ncol(pca_result$x))]
)

# Plot 1: Variance explained per PC
p1 <- ggplot(scree_data, aes(x = PC, y = Variance)) +
  geom_bar(stat = "identity", fill = "#2E86AB", alpha = 0.8) +
  geom_line(color = "#06668D", size = 1) +
  geom_point(color = "#06668D", size = 2) +
  labs(title = "Scree Plot - Variance Explained by Each PC",
       x = "Principal Component",
       y = "% Variance Explained") +
  theme_minimal() +
  theme(plot.title = element_text(hjust = 0.5, face = "bold"))

# Plot 2: Cumulative variance
p2 <- ggplot(scree_data, aes(x = PC, y = Cumulative)) +
  geom_line(color = "#C73E1D", size = 1.2) +
  geom_point(color = "#C73E1D", size = 2) +
  geom_hline(yintercept = 80, linetype = "dashed", color = "gray50") +
  labs(title = "Cumulative Variance Explained",
       x = "Principal Component",
       y = "Cumulative % Variance") +
  theme_minimal() +
  theme(plot.title = element_text(hjust = 0.5, face = "bold"))

grid.arrange(p1, p2, ncol = 2)

cat("\nVariance explained by first 10 PCs:\n")
print(scree_data[1:10, ])
```

## PCA Biplot - Loadings

```{r loadings-plot, fig.width=12, fig.height=10}
# Get all loadings (all PCs)
all_loadings <- as.data.frame(pca_result$rotation)
all_loadings <- data.frame(biomarker = rownames(all_loadings), all_loadings, row.names = NULL)

# Get loadings for PC1 and PC2 for plotting
loadings_plot_pc12 <- all_loadings[, c("biomarker", "PC1", "PC2")]

# Plot loadings PC1 vs PC2
ggplot(loadings_plot_pc12, aes(x = PC1, y = PC2)) +
  geom_segment(aes(xend = PC1, yend = PC2),
               x = 0, y = 0,
               arrow = arrow(length = unit(0.2, "cm")),
               color = "gray50", alpha = 0.5) +
  geom_point(color = "#C73E1D", size = 2) +
  geom_text(aes(label = biomarker),
            size = 2.5,
            hjust = -0.1,
            vjust = -0.1,
            check_overlap = TRUE) +
  labs(title = "PCA Loadings Plot - PC1 vs PC2",
       x = paste0("PC1 (", round(variance_explained[1], 2), "%)"),
       y = paste0("PC2 (", round(variance_explained[2], 2), "%)")) +
  theme_minimal() +
  theme(plot.title = element_text(hjust = 0.5, face = "bold"))

# Top contributing biomarkers to PC1 and PC2
cat("\nTop 10 biomarkers contributing to PC1:\n")
top_pc1 <- loadings_plot_pc12[order(abs(loadings_plot_pc12$PC1), decreasing = TRUE), ][1:10, ]
print(top_pc1)

cat("\nTop 10 biomarkers contributing to PC2:\n")
top_pc2 <- loadings_plot_pc12[order(abs(loadings_plot_pc12$PC2), decreasing = TRUE), ][1:10, ]
print(top_pc2)
```

## PCA Biplot - Loadings PC3 vs PC4

```{r loadings-plot-pc34, fig.width=12, fig.height=10}
# Get loadings for PC3 and PC4 for plotting
loadings_plot_pc34 <- all_loadings[, c("biomarker", "PC3", "PC4")]

# Plot loadings PC3 vs PC4
ggplot(loadings_plot_pc34, aes(x = PC3, y = PC4)) +
  geom_segment(aes(xend = PC3, yend = PC4),
               x = 0, y = 0,
               arrow = arrow(length = unit(0.2, "cm")),
               color = "gray50", alpha = 0.5) +
  geom_point(color = "#2E86AB", size = 2) +
  geom_text(aes(label = biomarker),
            size = 2.5,
            hjust = -0.1,
            vjust = -0.1,
            check_overlap = TRUE) +
  labs(title = "PCA Loadings Plot - PC3 vs PC4",
       x = paste0("PC3 (", round(variance_explained[3], 2), "%)"),
       y = paste0("PC4 (", round(variance_explained[4], 2), "%)")) +
  theme_minimal() +
  theme(plot.title = element_text(hjust = 0.5, face = "bold"))

# Top contributing biomarkers to PC3 and PC4
cat("\nTop 10 biomarkers contributing to PC3:\n")
top_pc3 <- loadings_plot_pc34[order(abs(loadings_plot_pc34$PC3), decreasing = TRUE), ][1:10, ]
print(top_pc3)

cat("\nTop 10 biomarkers contributing to PC4:\n")
top_pc4 <- loadings_plot_pc34[order(abs(loadings_plot_pc34$PC4), decreasing = TRUE), ][1:10, ]
print(top_pc4)
```

## PCA Plots - PC1 vs PC2

### By Clinical Diagnosis (CDX)

```{r pca-cdx, fig.width=10, fig.height=8}
if("CDX" %in% colnames(pca_data)) {
  ggplot(pca_data, aes(x = PC1, y = PC2, color = CDX, shape = CDX)) +
    geom_point(size = 3, alpha = 0.7) +
    stat_ellipse(level = 0.95, linetype = 2) +
    labs(title = "PCA Plot - Colored by Clinical Diagnosis",
         x = paste0("PC1 (", round(variance_explained[1], 2), "% variance)"),
         y = paste0("PC2 (", round(variance_explained[2], 2), "% variance)"),
         color = "Diagnosis",
         shape = "Diagnosis") +
    theme_minimal() +
    theme(plot.title = element_text(hjust = 0.5, face = "bold"),
          legend.position = "right")
}
```

### By Race/Ethnicity

```{r pca-race, fig.width=10, fig.height=8}
if("Race_Ethnicity" %in% colnames(pca_data)) {
  ggplot(pca_data, aes(x = PC1, y = PC2, color = Race_Ethnicity, shape = Race_Ethnicity)) +
    geom_point(size = 3, alpha = 0.7) +
    stat_ellipse(level = 0.95, linetype = 2) +
    labs(title = "PCA Plot - Colored by Race/Ethnicity",
         x = paste0("PC1 (", round(variance_explained[1], 2), "% variance)"),
         y = paste0("PC2 (", round(variance_explained[2], 2), "% variance)"),
         color = "Race/Ethnicity",
         shape = "Race/Ethnicity") +
    theme_minimal() +
    theme(plot.title = element_text(hjust = 0.5, face = "bold"),
          legend.position = "right") +
    scale_shape_manual(values = c(15:20, 0:14)[1:length(unique(pca_data$Race_Ethnicity))])
}
```

### By Run

```{r pca-run, fig.width=10, fig.height=8}
if("Run" %in% colnames(pca_data)) {
  ggplot(pca_data, aes(x = PC1, y = PC2, color = Run, shape = Run)) +
    geom_point(size = 3, alpha = 0.7) +
    stat_ellipse(level = 0.95, linetype = 2) +
    labs(title = "PCA Plot - Colored by Run",
         x = paste0("PC1 (", round(variance_explained[1], 2), "% variance)"),
         y = paste0("PC2 (", round(variance_explained[2], 2), "% variance)"),
         color = "Run",
         shape = "Run") +
    theme_minimal() +
    theme(plot.title = element_text(hjust = 0.5, face = "bold"),
          legend.position = "right") +
    scale_shape_manual(values = c(15:20, 0:14)[1:length(unique(pca_data$Run))])
}
```

### By Bay

```{r pca-Bay, fig.width=10, fig.height=8}
if("Bay" %in% colnames(pca_data)) {
  ggplot(pca_data, aes(x = PC1, y = PC2, color = Bay, shape = Bay)) +
    geom_point(size = 3, alpha = 0.7) +
    stat_ellipse(level = 0.95, linetype = 2) +
    labs(title = "PCA Plot - Colored by Bay",
         x = paste0("PC1 (", round(variance_explained[1], 2), "% variance)"),
         y = paste0("PC2 (", round(variance_explained[2], 2), "% variance)"),
         color = "Bay",
         shape = "Bay") +
    theme_minimal() +
    theme(plot.title = element_text(hjust = 0.5, face = "bold"),
          legend.position = "right") +
    scale_shape_manual(values = c(15:20, 0:14)[1:length(unique(pca_data$Bay))])
}
```

### By Sex

```{r pca-sex, fig.width=10, fig.height=8}
if("sex" %in% colnames(pca_data)) {
  ggplot(pca_data, aes(x = PC1, y = PC2, color = sex, shape = sex)) +
    geom_point(size = 3, alpha = 0.7) +
    stat_ellipse(level = 0.95, linetype = 2) +
    labs(title = "PCA Plot - Colored by Sex",
         x = paste0("PC1 (", round(variance_explained[1], 2), "% variance)"),
         y = paste0("PC2 (", round(variance_explained[2], 2), "% variance)"),
         color = "Sex",
         shape = "Sex") +
    theme_minimal() +
    theme(plot.title = element_text(hjust = 0.5, face = "bold"),
          legend.position = "right")
}
```

### By APOE Genotype

```{r pca-apoe, fig.width=10, fig.height=8, eval=FALSE}
if("APOE.geno" %in% colnames(pca_data)) {
  # Clean APOE genotype column (remove extra spaces)
  pca_data$APOE.geno_clean <- gsub("\\s+", "/", trimws(pca_data$APOE.geno))

  ggplot(pca_data, aes(x = PC1, y = PC2, color = APOE.geno_clean, shape = APOE.geno_clean)) +
    geom_point(size = 3, alpha = 0.7) +
    labs(title = "PCA Plot - Colored by APOE Genotype",
         x = paste0("PC1 (", round(variance_explained[1], 2), "% variance)"),
         y = paste0("PC2 (", round(variance_explained[2], 2), "% variance)"),
         color = "APOE Genotype",
         shape = "APOE Genotype") +
    theme_minimal() +
    theme(plot.title = element_text(hjust = 0.5, face = "bold"),
          legend.position = "right") +
    scale_shape_manual(values = c(15:20, 0:14)[1:length(unique(pca_data$APOE.geno_clean))])
}
```

### By Age (Continuous)

```{r pca-age, fig.width=10, fig.height=8}
if("age_at_sample" %in% colnames(pca_data)) {
  pca_data$age_numeric <- as.numeric(pca_data$age_at_sample)

  ggplot(pca_data, aes(x = PC1, y = PC2, color = age_numeric)) +
    geom_point(size = 3, alpha = 0.7) +
    scale_color_gradient2(low = "#06668D", mid = "#F0E442", high = "#C73E1D",
                         midpoint = median(pca_data$age_numeric, na.rm = TRUE)) +
    labs(title = "PCA Plot - Colored by Age at Sample",
         x = paste0("PC1 (", round(variance_explained[1], 2), "% variance)"),
         y = paste0("PC2 (", round(variance_explained[2], 2), "% variance)"),
         color = "Age") +
    theme_minimal() +
    theme(plot.title = element_text(hjust = 0.5, face = "bold"),
          legend.position = "right")
}
```

### By Site

```{r pca-site, fig.width=10, fig.height=8}
if("Site" %in% colnames(pca_data)) {
  ggplot(pca_data, aes(x = PC1, y = PC2, color = Site, shape = Site)) +
    geom_point(size = 3, alpha = 0.7) +
    labs(title = "PCA Plot - Colored by Site",
         x = paste0("PC1 (", round(variance_explained[1], 2), "% variance)"),
         y = paste0("PC2 (", round(variance_explained[2], 2), "% variance)"),
         color = "Site",
         shape = "Site") +
    theme_minimal() +
    theme(plot.title = element_text(hjust = 0.5, face = "bold"),
          legend.position = "right") +
    scale_shape_manual(values = c(15:20, 0:14)[1:length(unique(pca_data$Site))])
}
```

## PCA Plots - PC2 vs PC3

```{r pca-race23, fig.width=10, fig.height=8, eval=FALSE}
if("Race_Ethnicity" %in% colnames(pca_data)) {
  ggplot(pca_data, aes(x = PC2, y = PC3, color = Race_Ethnicity, shape = Race_Ethnicity)) +
    geom_point(size = 3, alpha = 0.7) +
    stat_ellipse(level = 0.95, linetype = 2) +
    labs(title = "PCA Plot - Colored by Race/Ethnicity",
         x = paste0("PC2 (", round(variance_explained[2], 2), "% variance)"),
         y = paste0("PC3 (", round(variance_explained[3], 2), "% variance)"),
         color = "Race/Ethnicity",
         shape = "Race/Ethnicity") +
    theme_minimal() +
    theme(plot.title = element_text(hjust = 0.5, face = "bold"),
          legend.position = "right") +
    scale_shape_manual(values = c(15:20, 0:14)[1:length(unique(pca_data$Race_Ethnicity))])
}
```


## Custom PCA Plots

```{r custom-function}
# Function to create custom PCA plots
plot_pca_custom <- function(pca_data, pc_x = "PC1", pc_y = "PC2",
                           color_var = NULL, shape_var = NULL,
                           add_ellipse = TRUE, point_size = 3, alpha = 0.7) {

  # Get variance explained for the selected PCs
  pc_x_num <- as.numeric(gsub("PC", "", pc_x))
  pc_y_num <- as.numeric(gsub("PC", "", pc_y))

  var_x <- round(variance_explained[pc_x_num], 2)
  var_y <- round(variance_explained[pc_y_num], 2)

  # Create base plot
  p <- ggplot(pca_data, aes_string(x = pc_x, y = pc_y))

  # Add color and shape aesthetics if specified
  if (!is.null(color_var) && !is.null(shape_var)) {
    p <- p + geom_point(aes_string(color = color_var, shape = shape_var),
                       size = point_size, alpha = alpha)
    if (add_ellipse && is.character(pca_data[[color_var]])) {
      p <- p + stat_ellipse(aes_string(color = color_var), level = 0.95, linetype = 2)
    }
  } else if (!is.null(color_var)) {
    p <- p + geom_point(aes_string(color = color_var),
                       size = point_size, alpha = alpha)
    if (add_ellipse && is.character(pca_data[[color_var]])) {
      p <- p + stat_ellipse(aes_string(color = color_var), level = 0.95, linetype = 2)
    }
  } else if (!is.null(shape_var)) {
    p <- p + geom_point(aes_string(shape = shape_var),
                       size = point_size, alpha = alpha)
  } else {
    p <- p + geom_point(size = point_size, alpha = alpha)
  }

  # Add labels and theme
  p <- p +
    labs(x = paste0(pc_x, " (", var_x, "% variance)"),
         y = paste0(pc_y, " (", var_y, "% variance)")) +
    theme_minimal() +
    theme(plot.title = element_text(hjust = 0.5, face = "bold"),
          legend.position = "right")

  return(p)
}
```

### Custom Plots

```{r custom-examples, fig.width=10, fig.height=8, eval=FALSE}
# Example 1: PC3 vs PC4, colored by CDX, shaped by sex
# if("CDX" %in% colnames(pca_data) && "sex" %in% colnames(pca_data)) {
#   p1 <- plot_pca_custom(pca_data, pc_x = "PC1", pc_y = "PC2",
#                        color_var = "CDX", shape_var = "sex")
#   p1 <- p1 + labs(title = "Custom PCA Plot: CDX (color) and Sex (shape)")
#   print(p1)
# }


 p1 <- plot_pca_custom(pca_data, pc_x = "PC1", pc_y = "PC2",
                       color_var = "Site", shape_var = "Bay")
print(p1)
  p2 <- plot_pca_custom(pca_data, pc_x = "PC3", pc_y = "PC4",
                        color_var = "Site", shape_var = "Bay")
print(p2)
  p3 <- plot_pca_custom(pca_data, pc_x = "PC5", pc_y = "PC6",
                        color_var = "Site", shape_var = "Bay")
print(p3)

  p4 <- plot_pca_custom(pca_data, pc_x = "PC7", pc_y = "PC8",
                        color_var = "Site", shape_var = "Bay")
print(p4)
  p5 <- plot_pca_custom(pca_data, pc_x = "PC9", pc_y = "PC10",
                        color_var = "Site", shape_var = "Bay")
  print(p5)
```

## Multi-Panel PCA Plot

```{r multi-panel, fig.width=12, fig.height=14, eval=FALSE}
# Create a multi-panel plot showing multiple metadata variables
if(all(c("CDX", "sex", "Race_Ethnicity", "Site") %in% colnames(pca_data))) {
  p1 <- plot_pca_custom(pca_data, pc_x = "PC1", pc_y = "PC2",
                       color_var = "Site", shape_var = "CDX")

  p2 <- plot_pca_custom(pca_data, pc_x = "PC3", pc_y = "PC4",
                        color_var = "Site", shape_var = "CDX")

  p3 <- plot_pca_custom(pca_data, pc_x = "PC5", pc_y = "PC6",
                        color_var = "Site", shape_var = "CDX")


  p4 <- plot_pca_custom(pca_data, pc_x = "PC7", pc_y = "PC8",
                        color_var = "Site", shape_var = "CDX")

  p5 <- plot_pca_custom(pca_data, pc_x = "PC9", pc_y = "PC10",
                        color_var = "Site", shape_var = "CDX")

  grid.arrange(p1, p2, p3, p4, p5,  ncol = 2)
}
```

# Analysis

## Metadata Correlations

```{r metadata-correlations, fig.width=12, fig.height=10}
# Analyze correlations between metadata variables
# This helps identify which metadata variables are related to each other

cat("\n### Correlations Between Metadata Variables\n\n")

# Prepare metadata for correlation analysis
metadata_for_corr <- metadata_complete[, VARS_TO_TEST]

# Function to calculate association between two variables
calculate_metadata_association <- function(var1, var2, data, min_n = MIN_GROUP_SIZE) {

  v1 <- data[[var1]]
  v2 <- data[[var2]]

  # Remove missing values
  valid_idx <- !is.na(v1) & !is.na(v2)
  v1_clean <- v1[valid_idx]
  v2_clean <- v2[valid_idx]

  if (length(v1_clean) < 10) return(list(stat = NA, pval = NA, type = NA))

  # Determine variable types
  is_v1_numeric <- is.numeric(v1_clean) && length(unique(v1_clean)) > 5
  is_v2_numeric <- is.numeric(v2_clean) && length(unique(v2_clean)) > 5

  if (is_v1_numeric && is_v2_numeric) {
    # Both numeric: Pearson correlation
    test <- cor.test(v1_clean, v2_clean)
    return(list(stat = test$estimate, pval = test$p.value, type = "Correlation (r)"))

  } else if (!is_v1_numeric && !is_v2_numeric) {
    # Both categorical: Cramér's V
    # Filter small groups
    v1_counts <- table(v1_clean)
    v2_counts <- table(v2_clean)
    v1_keep <- names(v1_counts[v1_counts >= min_n])
    v2_keep <- names(v2_counts[v2_counts >= min_n])

    keep_idx <- v1_clean %in% v1_keep & v2_clean %in% v2_keep

    if (sum(keep_idx) < 10) return(list(stat = NA, pval = NA, type = NA))

    v1_test <- factor(v1_clean[keep_idx])
    v2_test <- factor(v2_clean[keep_idx])

    # Check if there are at least 2 levels in each variable
    if (length(levels(v1_test)) < 2 || length(levels(v2_test)) < 2) {
      return(list(stat = NA, pval = NA, type = NA))
    }

    # Chi-square test
    contingency <- table(v1_test, v2_test)

    # Check if contingency table has enough cells
    if (nrow(contingency) < 2 || ncol(contingency) < 2) {
      return(list(stat = NA, pval = NA, type = NA))
    }

    # Try chi-square test with error handling
    chi_test <- tryCatch(
      chisq.test(contingency),
      error = function(e) return(NULL)
    )

    if (is.null(chi_test)) {
      return(list(stat = NA, pval = NA, type = NA))
    }

    # Cramér's V
    n <- sum(contingency)
    min_dim <- min(nrow(contingency) - 1, ncol(contingency) - 1)
    cramers_v <- sqrt(chi_test$statistic / (n * min_dim))

    return(list(stat = as.numeric(cramers_v), pval = chi_test$p.value, type = "Cramér's V"))

  } else {
    # One numeric, one categorical: Eta-squared from ANOVA
    if (is_v1_numeric) {
      num_var <- v1_clean
      cat_var <- as.character(v2_clean)
    } else {
      num_var <- v2_clean
      cat_var <- as.character(v1_clean)
    }

    # Filter small groups
    cat_counts <- table(cat_var)
    cat_keep <- names(cat_counts[cat_counts >= min_n])

    if (length(cat_keep) < 2) return(list(stat = NA, pval = NA, type = NA))

    keep_idx <- cat_var %in% cat_keep
    num_test <- num_var[keep_idx]
    cat_test <- factor(cat_var[keep_idx])

    # ANOVA
    kw_test <- kruskal.test(num_test ~ cat_test)

    # Eta-squared
    group_means <- tapply(num_test, cat_test, mean)
    grand_mean <- mean(num_test)
    ss_between <- sum(table(cat_test) * (group_means - grand_mean)^2)
    ss_total <- sum((num_test - grand_mean)^2)
    eta_sq <- ss_between / ss_total

    return(list(stat = eta_sq, pval = kw_test$p.value, type = "Eta-squared (η²)"))
  }
}

# Calculate all pairwise associations
n_vars <- length(VARS_TO_TEST)
corr_matrix <- matrix(NA, n_vars, n_vars)
pval_matrix <- matrix(NA, n_vars, n_vars)
type_matrix <- matrix(NA, n_vars, n_vars)
rownames(corr_matrix) <- colnames(corr_matrix) <- VARS_TO_TEST
rownames(pval_matrix) <- colnames(pval_matrix) <- VARS_TO_TEST
rownames(type_matrix) <- colnames(type_matrix) <- VARS_TO_TEST

for (i in 1:n_vars) {
  for (j in 1:n_vars) {
    if (i == j) {
      corr_matrix[i, j] <- 1
      pval_matrix[i, j] <- 0
      type_matrix[i, j] <- "Self"
    } else if (i < j) {
      result <- calculate_metadata_association(VARS_TO_TEST[i], VARS_TO_TEST[j],
                                                metadata_for_corr, MIN_GROUP_SIZE)
      corr_matrix[i, j] <- corr_matrix[j, i] <- result$stat
      pval_matrix[i, j] <- pval_matrix[j, i] <- result$pval
      type_matrix[i, j] <- type_matrix[j, i] <- result$type
    }
  }
}

# Create data frame for plotting
corr_long <- expand.grid(Var1 = VARS_TO_TEST, Var2 = VARS_TO_TEST)
corr_long$Association <- as.vector(corr_matrix)
corr_long$Pvalue <- as.vector(pval_matrix)
corr_long$Type <- as.vector(type_matrix)

# Add significance markers
corr_long <- corr_long %>%
  mutate(
    Sig = case_when(
      is.na(Pvalue) ~ "",
      Pvalue < 0.001 ~ "***",
      Pvalue < 0.01 ~ "**",
      Pvalue < 0.05 ~ "*",
      TRUE ~ ""
    ),
    # For display: only show lower triangle
    Display = ifelse(Var1 == Var2 | is.na(Association), NA, Association)
  )

# Plot correlation heatmap
p_corr <- ggplot(corr_long, aes(x = Var1, y = Var2, fill = Association)) +
  geom_tile(color = "white", linewidth = 0.5) +
  geom_text(aes(label = Sig), size = 4, vjust = 0.75) +
  scale_fill_gradient2(
    low = "#06668D",
    mid = "white",
    high = "#C73E1D",
    midpoint = 0.5,
    limits = c(0, 1),
    na.value = "gray90",
    name = "Association\nStrength"
  ) +
  labs(
    title = "Associations Between Metadata Variables",
    subtitle = "Colors show association strength (Correlation/Cramér's V/η²); * p<0.05, ** p<0.01, *** p<0.001",
    x = "",
    y = ""
  ) +
  theme_minimal() +
  theme(
    plot.title = element_text(hjust = 0.5, face = "bold", size = 14),
    plot.subtitle = element_text(hjust = 0.5, size = 10),
    axis.text.x = element_text(angle = 45, hjust = 1, size = 10),
    axis.text.y = element_text(size = 10),
    legend.position = "right",
    panel.grid = element_blank()
  )

print(p_corr)

# Create summary table of strongest associations
cat("\n\n### Strongest Metadata Associations\n\n")

corr_summary <- corr_long %>%
  filter(Var1 != Var2, !is.na(Association)) %>%
  filter(as.character(Var1) < as.character(Var2)) %>%  # Only lower triangle
  arrange(desc(Association)) %>%
  head(20) %>%
  select(Var1, Var2, Type, Association, Pvalue) %>%
  mutate(
    Association = round(Association, 3),
    Pvalue = format.pval(Pvalue, digits = 3, eps = 0.001)
  )

print(knitr::kable(corr_summary, row.names = FALSE,
                   col.names = c("Variable 1", "Variable 2", "Test Type",
                                 "Strength", "P-value")))

# Save results
corr_results <- corr_long %>%
  filter(Var1 != Var2, !is.na(Association)) %>%
  filter(as.character(Var1) < as.character(Var2)) %>%
  arrange(desc(Association))

write.csv(corr_results, "metadata_correlations.csv", row.names = FALSE)
cat("\n**Results saved to:** metadata_correlations.csv\n")

cat("\n\n### Interpretation:\n")
cat("- Values range from 0 (no association) to 1 (perfect association)\n")
cat("- **Correlation (r)**: Used for two continuous variables (e.g., age)\n")
cat("- **Cramér's V**: Used for two categorical variables (e.g., Site vs CDX)\n")
cat("- **Eta-squared (η²)**: Used for one continuous and one categorical variable\n")
cat("- High correlations between metadata variables may indicate confounding\n")
```

## Association Tests - Metadata vs PCs

```{r association-tests}
# Test associations between metadata variables and PCs
# Uses configuration from earlier: MIN_GROUP_SIZE, N_PCS_TO_TEST, VARS_TO_TEST

# Prepare data for testing
test_data <- cbind(metadata_complete, pca_scores)

# Function to test associations
test_associations <- function(data, var_name, pc_names, min_n = MIN_GROUP_SIZE) {

  # Skip if variable not in data
  if (!var_name %in% colnames(data)) {
    return(NULL)
  }

  # Get the variable
  var_data <- data[[var_name]]

  # Skip if all missing
  if (all(is.na(var_data))) {
    return(NULL)
  }

  # Check if variable is numeric or categorical
  is_numeric <- is.numeric(var_data) && length(unique(na.omit(var_data))) > 5

  results <- data.frame()

  for (pc in pc_names) {
    pc_values <- data[[pc]]

    if (is_numeric) {
      # Correlation test for continuous variables
      valid_idx <- !is.na(var_data) & !is.na(pc_values)
      if (sum(valid_idx) >= 10) {
        cor_test <- cor.test(var_data[valid_idx], pc_values[valid_idx])
        results <- rbind(results, data.frame(
          Variable = var_name,
          PC = pc,
          Test = "Correlation",
          Statistic = cor_test$estimate,
          P_value = cor_test$p.value,
          N = sum(valid_idx)
        ))
      }
    } else {
      # ANOVA/Kruskal-Wallis for categorical variables
      # Remove NA and filter small groups
      valid_idx <- !is.na(var_data) & !is.na(pc_values)
      var_clean <- as.character(var_data[valid_idx])
      pc_clean <- pc_values[valid_idx]

      # Count group sizes
      group_counts <- table(var_clean)
      large_groups <- names(group_counts[group_counts >= min_n])

      # Filter to only large groups
      if (length(large_groups) >= 2) {
        keep_idx <- var_clean %in% large_groups
        var_test <- factor(var_clean[keep_idx])
        pc_test <- pc_clean[keep_idx]

        # Perform Kruskal-Wallis test (non-parametric)
        kw_test <- kruskal.test(pc_test ~ var_test)

        results <- rbind(results, data.frame(
          Variable = var_name,
          PC = pc,
          Test = paste0("Kruskal-Wallis (", length(large_groups), " groups)"),
          Statistic = kw_test$statistic,
          P_value = kw_test$p.value,
          N = length(pc_test)
        ))
      }
    }
  }

  return(results)
}

# Get PC names to test
pc_names_test <- paste0("PC", 1:min(N_PCS_TO_TEST, ncol(pca_scores)))

# Run tests for all variables
all_results <- data.frame()
for (var in VARS_TO_TEST) {
  var_results <- test_associations(test_data, var, pc_names_test, MIN_GROUP_SIZE)
  if (!is.null(var_results)) {
    all_results <- rbind(all_results, var_results)
  }
}

# Add adjusted p-values (Bonferroni correction)
if (nrow(all_results) > 0) {
  all_results$P_adjusted <- p.adjust(all_results$P_value, method = "bonferroni")

  # Format columns for display
  all_results$Statistic <- round(all_results$Statistic, 4)
  all_results$P_value <- format.pval(all_results$P_value, digits = 3, eps = 0.001)
  all_results$P_adjusted <- format.pval(all_results$P_adjusted, digits = 3, eps = 0.001)

  # Sort by adjusted p-value
  all_results <- all_results[order(as.numeric(gsub("<", "", all_results$P_adjusted))), ]

  cat("\n### Association Tests Summary\n\n")
  cat("**Minimum group size:** ", MIN_GROUP_SIZE, "\n\n")
  cat("**Number of tests performed:** ", nrow(all_results), "\n\n")

  # Show significant results (adjusted p < 0.05)
  sig_results <- all_results[as.numeric(gsub("<", "", all_results$P_adjusted)) < 0.05, ]

  if (nrow(sig_results) > 0) {
    cat("\n### Significant Associations (adjusted p < 0.05)\n\n")
    knitr::kable(sig_results, row.names = FALSE, align = "lllrrr",
                 caption = "Significant associations between metadata variables and principal components") %>%
      print()
  } else {
    cat("\n**No significant associations found at adjusted p < 0.05**\n\n")
  }

  # Show top 20 results
  cat("\n### Top 20 Associations (by adjusted p-value)\n\n")
  knitr::kable(head(all_results, 20), row.names = FALSE, align = "lllrrr",
               caption = "Top 20 associations ranked by adjusted p-value") %>%
    print()

  # Save results (with numeric p-values for downstream analysis)
  all_results_save <- all_results
  all_results_save$P_value <- as.numeric(gsub("<", "", all_results$P_value))
  all_results_save$P_adjusted <- as.numeric(gsub("<", "", all_results$P_adjusted))
  write.csv(all_results_save, "pca_metadata_associations.csv", row.names = FALSE)
  cat("\n**Full results saved to:** pca_metadata_associations.csv\n")
}
```

## Heatmap of Metadata-PC Associations

```{r association-heatmap, fig.width=12, fig.height=8}
# Create a heatmap showing strength of associations between metadata and PCs

if (exists("all_results_save") && nrow(all_results_save) > 0) {

  # Prepare data for heatmap - use -log10(p-value) for visualization
  # Higher values = stronger associations
  heatmap_data <- all_results_save %>%
    mutate(neg_log_p = -log10(P_adjusted + 1e-300)) %>%  # Add small value to avoid log(0)
    select(Variable, PC, neg_log_p)

  # Convert to wide format for heatmap
  heatmap_wide <- heatmap_data %>%
    pivot_wider(names_from = PC, values_from = neg_log_p, values_fill = 0)

  # Convert to matrix
  heatmap_matrix <- as.matrix(heatmap_wide[, -1])
  rownames(heatmap_matrix) <- heatmap_wide$Variable

  # Create heatmap using ggplot2
  heatmap_long <- heatmap_data %>%
    mutate(PC_num = as.numeric(gsub("PC", "", PC))) %>%
    filter(PC_num <= 10)  # Show first 10 PCs for clarity

  # Add significance markers
  heatmap_long <- heatmap_long %>%
    left_join(all_results_save %>%
                select(Variable, PC, P_adjusted),
              by = c("Variable", "PC")) %>%
    mutate(sig_marker = case_when(
      P_adjusted < 0.001 ~ "***",
      P_adjusted < 0.01 ~ "**",
      P_adjusted < 0.05 ~ "*",
      TRUE ~ ""
    ))

  p <- ggplot(heatmap_long, aes(x = PC, y = Variable, fill = neg_log_p)) +
    geom_tile(color = "white", linewidth = 0.5) +
    geom_text(aes(label = sig_marker), size = 4, vjust = 0.75) +
    scale_fill_gradient2(
      low = "white",
      mid = "#F0E442",
      high = "#C73E1D",
      midpoint = -log10(0.05),
      name = "-log10(p.adj)",
      limits = c(0, max(heatmap_long$neg_log_p, na.rm = TRUE))
    ) +
    labs(
      title = "Association Strength: Metadata Variables vs Principal Components",
      subtitle = "Color intensity = -log10(adjusted p-value); * p<0.05, ** p<0.01, *** p<0.001",
      x = "Principal Component",
      y = "Metadata Variable"
    ) +
    theme_minimal() +
    theme(
      plot.title = element_text(hjust = 0.5, face = "bold", size = 14),
      plot.subtitle = element_text(hjust = 0.5, size = 10),
      axis.text.x = element_text(angle = 0, hjust = 0.5),
      axis.text.y = element_text(size = 10),
      legend.position = "right",
      panel.grid = element_blank()
    ) +
    geom_hline(yintercept = seq(1.5, length(unique(heatmap_long$Variable)) - 0.5, 1),
               color = "gray80", linewidth = 0.3)

  print(p)

  cat("\n\nHeatmap interpretation:\n")
  cat("- Darker red colors indicate stronger associations (more significant)\n")
  cat("- Horizontal line at -log10(0.05) =", round(-log10(0.05), 2), "indicates significance threshold\n")
  cat("- Values above this line are significant after multiple testing correction\n\n")

} else {
  cat("\nNo association test results available for heatmap\n")
}
```

## Variance Partitioning - How Much Variance Does Each Metadata Variable Explain?

```{r variance-partitioning, fig.width=12, fig.height=10}
# Calculate R² (effect size) for each metadata variable on each PC
# This shows what proportion of variance in each PC is explained by each metadata variable

if (nrow(all_results) > 0) {

  # Function to calculate effect size (R² or eta²)
  calculate_effect_size <- function(data, var_name, pc_name, min_n = MIN_GROUP_SIZE) {

    if (!var_name %in% colnames(data) || !pc_name %in% colnames(data)) {
      return(NA)
    }

    var_data <- data[[var_name]]
    pc_values <- data[[pc_name]]

    # Skip if all missing
    if (all(is.na(var_data)) || all(is.na(pc_values))) {
      return(NA)
    }

    # Check if variable is numeric or categorical
    is_numeric <- is.numeric(var_data) && length(unique(na.omit(var_data))) > 5

    if (is_numeric) {
      # R² from correlation for continuous variables
      valid_idx <- !is.na(var_data) & !is.na(pc_values)
      if (sum(valid_idx) >= 10) {
        r <- cor(var_data[valid_idx], pc_values[valid_idx])
        return(r^2)
      }
    } else {
      # Eta-squared (η²) for categorical variables
      valid_idx <- !is.na(var_data) & !is.na(pc_values)
      var_clean <- as.character(var_data[valid_idx])
      pc_clean <- pc_values[valid_idx]

      # Count group sizes and filter
      group_counts <- table(var_clean)
      large_groups <- names(group_counts[group_counts >= min_n])

      if (length(large_groups) >= 2) {
        keep_idx <- var_clean %in% large_groups
        var_test <- factor(var_clean[keep_idx])
        pc_test <- pc_clean[keep_idx]

        # Calculate eta-squared (between-group variance / total variance)
        group_means <- tapply(pc_test, var_test, mean)
        grand_mean <- mean(pc_test)

        # Between-group sum of squares
        ss_between <- sum(table(var_test) * (group_means - grand_mean)^2)

        # Total sum of squares
        ss_total <- sum((pc_test - grand_mean)^2)

        # Eta-squared
        eta_sq <- ss_between / ss_total
        return(eta_sq)
      }
    }

    return(NA)
  }

  # Calculate effect sizes for all variable-PC combinations
  effect_size_results <- data.frame()

  for (var in VARS_TO_TEST) {
    for (pc in pc_names_test) {
      effect_size <- calculate_effect_size(test_data, var, pc, MIN_GROUP_SIZE)

      if (!is.na(effect_size)) {
        effect_size_results <- rbind(effect_size_results, data.frame(
          Variable = var,
          PC = pc,
          Effect_Size = effect_size
        ))
      }
    }
  }

  # Add p-values from association tests
  effect_size_results <- effect_size_results %>%
    left_join(all_results_save %>% select(Variable, PC, P_adjusted),
              by = c("Variable", "PC"))

  # Convert to percentage
  effect_size_results$Variance_Explained_Pct <- effect_size_results$Effect_Size * 100

  # Create heatmap for first 10 PCs
  heatmap_data_es <- effect_size_results %>%
    mutate(PC_num = as.numeric(gsub("PC", "", PC))) %>%
    filter(PC_num <= 10) %>%
    mutate(sig_marker = case_when(
      P_adjusted < 0.001 ~ "***",
      P_adjusted < 0.01 ~ "**",
      P_adjusted < 0.05 ~ "*",
      TRUE ~ ""
    ))

  p_effect <- ggplot(heatmap_data_es, aes(x = PC, y = Variable, fill = Variance_Explained_Pct)) +
    geom_tile(color = "white", linewidth = 0.5) +
    geom_text(aes(label = paste0(round(Variance_Explained_Pct, 1), "\n", sig_marker)),
              size = 3, vjust = 0.5) +
    scale_fill_gradient2(
      low = "white",
      mid = "#A0C4E0",
      high = "#06668D",
      midpoint = 10,
      name = "% Variance\nExplained",
      limits = c(0, max(heatmap_data_es$Variance_Explained_Pct, na.rm = TRUE))
    ) +
    labs(
      title = "Variance Explained: How Much of Each PC is Explained by Each Metadata Variable?",
      subtitle = "Values show % variance explained (R² or η²); * p<0.05, ** p<0.01, *** p<0.001",
      x = "Principal Component",
      y = "Metadata Variable"
    ) +
    theme_minimal() +
    theme(
      plot.title = element_text(hjust = 0.5, face = "bold", size = 14),
      plot.subtitle = element_text(hjust = 0.5, size = 10),
      axis.text.x = element_text(angle = 0, hjust = 0.5),
      axis.text.y = element_text(size = 10),
      legend.position = "right",
      panel.grid = element_blank()
    ) +
    geom_hline(yintercept = seq(1.5, length(unique(heatmap_data_es$Variable)) - 0.5, 1),
               color = "gray80", linewidth = 0.3)

  print(p_effect)

  # Summary tables
  cat("\n\n### Summary: Top Metadata Variables Explaining Each PC\n\n")

  # For each of first 5 PCs, show top 3 variables
  for (pc_num in 1:5) {
    pc_name <- paste0("PC", pc_num)

    top_vars <- effect_size_results %>%
      filter(PC == pc_name) %>%
      arrange(desc(Variance_Explained_Pct)) %>%
      head(3)

    if (nrow(top_vars) > 0) {
      cat("\n**", pc_name, "** (", round(variance_explained[pc_num], 2), "% total variance):\n", sep = "")
      for (i in 1:nrow(top_vars)) {
        cat("  ", i, ". ", top_vars$Variable[i],
            ": ", round(top_vars$Variance_Explained_Pct[i], 2), "% ",
            "(p.adj = ", format.pval(top_vars$P_adjusted[i], digits = 2, eps = 0.01), ")\n", sep = "")
      }
    }
  }

  # Save results
  write.csv(effect_size_results, "pca_variance_partitioning.csv", row.names = FALSE)
  cat("\n\n**Variance partitioning results saved to:** pca_variance_partitioning.csv\n")

  cat("\n\n### Interpretation:\n")
  cat("- R² (continuous variables) or η² (categorical variables) shows proportion of variance explained\n")
  cat("- Values range from 0% (no association) to 100% (perfect association)\n")
  cat("- Effect size benchmarks: small (~1%), medium (~6%), large (~14%)\n")
  cat("- A variable can be statistically significant but explain little variance\n")

} else {
  cat("\nNo association test results available for variance partitioning\n")
}
```

## PCA Plots - All Significant Associations for Each Metadata Variable

```{r significant-pca-plots, fig.width=10, fig.height=8}
# Generate PCA plots showing all significant PC associations for each metadata variable
# Uses configuration: PLOT_P_THRESHOLD

if (exists("all_results_save") && nrow(all_results_save) > 0) {

  cat("\n### Visualizing All Significant PC Associations for Each Metadata Variable\n\n")
  cat("Plotting PCs with adjusted p-value <", PLOT_P_THRESHOLD, "\n\n")

  # Get unique variables that have been tested
  tested_vars <- unique(all_results_save$Variable)

  for (var_name in tested_vars) {

    # Get all significant associations for this variable, sorted by p-value
    var_assoc <- all_results_save %>%
      filter(Variable == var_name, P_adjusted < PLOT_P_THRESHOLD) %>%
      arrange(P_adjusted)

    if (nrow(var_assoc) == 0) {
      cat("\n**", var_name, "**: No significant associations (p.adj >= ", PLOT_P_THRESHOLD, ")\n", sep = "")
      next
    }

    # Print table of all significant associations for this variable
    cat("\n**", var_name, "**: ", nrow(var_assoc), " significant PC association(s)\n\n", sep = "")

    var_assoc_display <- var_assoc %>%
      select(PC, P_value, P_adjusted, Statistic, Test, N)

    # Add variance explained if available
    if (exists("effect_size_results")) {
      var_assoc_display <- var_assoc_display %>%
        left_join(effect_size_results %>%
                    filter(Variable == var_name) %>%
                    select(PC, Variance_Explained_Pct),
                  by = "PC")
    }

    print(knitr::kable(var_assoc_display, digits = 3, row.names = FALSE))
    cat("\n")

    # Create plots for pairs of significant PCs
    # Strategy: Create unique pairs to avoid redundant plots
    # For 6 significant PCs (PC1, PC3, PC5, PC7, PC9, PC11), create 3 plots:
    # PC1 vs PC3, PC5 vs PC7, PC9 vs PC11

    sig_pcs <- var_assoc$PC
    n_sig_pcs <- length(sig_pcs)

    # Create pairs
    if (n_sig_pcs == 1) {
      # Only one significant PC - plot against PC1 or PC2
      pc_pairs <- data.frame(
        pc_x = sig_pcs[1],
        pc_y = if (sig_pcs[1] == "PC1") "PC2" else "PC1"
      )
    } else {
      # Multiple significant PCs - create pairs
      # Pair them sequentially: (1st, 2nd), (3rd, 4th), (5th, 6th), etc.
      pc_pairs <- data.frame()
      i <- 1
      while (i <= n_sig_pcs) {
        if (i < n_sig_pcs) {
          # Pair with next significant PC
          pc_pairs <- rbind(pc_pairs, data.frame(
            pc_x = sig_pcs[i],
            pc_y = sig_pcs[i + 1]
          ))
          i <- i + 2
        } else {
          # Odd one out - pair with PC1 or most significant PC
          if (sig_pcs[i] != sig_pcs[1]) {
            pc_pairs <- rbind(pc_pairs, data.frame(
              pc_x = sig_pcs[i],
              pc_y = sig_pcs[1]
            ))
          } else {
            # If only PC1, pair with PC2
            pc_pairs <- rbind(pc_pairs, data.frame(
              pc_x = sig_pcs[i],
              pc_y = "PC2"
            ))
          }
          i <- i + 1
        }
      }
    }

    # Create plots for each pair
    for (pair_idx in 1:nrow(pc_pairs)) {
      pc_x <- pc_pairs$pc_x[pair_idx]
      pc_y <- pc_pairs$pc_y[pair_idx]

      # Check if variable and PCs exist in pca_data
      if (var_name %in% colnames(pca_data) &&
          pc_x %in% colnames(pca_data) &&
          pc_y %in% colnames(pca_data)) {

        pc_x_num <- as.numeric(gsub("PC", "", pc_x))
        pc_y_num <- as.numeric(gsub("PC", "", pc_y))

        # Get p-values and variance explained for both PCs
        pc_x_info <- var_assoc %>% filter(PC == pc_x)
        pc_y_info <- var_assoc %>% filter(PC == pc_y)

        # Create subtitle with association info
        subtitle_parts <- c()

        if (nrow(pc_x_info) > 0) {
          pc_x_pval <- format.pval(pc_x_info$P_adjusted[1], digits = 2, eps = 0.01)
          subtitle_parts <- c(subtitle_parts, paste0(pc_x, " p.adj=", pc_x_pval))

          if (exists("effect_size_results")) {
            var_exp_x <- effect_size_results %>%
              filter(Variable == var_name, PC == pc_x) %>%
              pull(Variance_Explained_Pct)
            if (length(var_exp_x) > 0) {
              subtitle_parts[length(subtitle_parts)] <- paste0(
                subtitle_parts[length(subtitle_parts)],
                ", R²=", round(var_exp_x, 1), "%"
              )
            }
          }
        }

        if (nrow(pc_y_info) > 0) {
          pc_y_pval <- format.pval(pc_y_info$P_adjusted[1], digits = 2, eps = 0.01)
          subtitle_parts <- c(subtitle_parts, paste0(pc_y, " p.adj=", pc_y_pval))

          if (exists("effect_size_results")) {
            var_exp_y <- effect_size_results %>%
              filter(Variable == var_name, PC == pc_y) %>%
              pull(Variance_Explained_Pct)
            if (length(var_exp_y) > 0) {
              subtitle_parts[length(subtitle_parts)] <- paste0(
                subtitle_parts[length(subtitle_parts)],
                ", R²=", round(var_exp_y, 1), "%"
              )
            }
          }
        }

        subtitle_text <- paste(subtitle_parts, collapse = "; ")
        if (subtitle_text == "") {
          subtitle_text <- "Significant PC associations"
        }

        # Create the plot
        p <- ggplot(pca_data, aes_string(x = pc_x, y = pc_y, color = var_name)) +
          geom_point(size = 3, alpha = 0.7) +
          labs(title = paste0(var_name, " - ", pc_x, " vs ", pc_y),
               subtitle = subtitle_text,
               x = paste0(pc_x, " (", round(variance_explained[pc_x_num], 2), "% total variance)"),
               y = paste0(pc_y, " (", round(variance_explained[pc_y_num], 2), "% total variance)"),
               color = var_name) +
          theme_minimal() +
          theme(plot.title = element_text(hjust = 0.5, face = "bold", size = 12),
                plot.subtitle = element_text(hjust = 0.5, size = 9),
                legend.position = "right")

        # Add ellipses for categorical variables
        if (!is.numeric(pca_data[[var_name]])) {
          p <- p + stat_ellipse(level = 0.95, linetype = 2)
        }

        print(p)
      }
    }
  }

  # Count total plots generated
  total_sig <- sum(all_results_save$P_adjusted < PLOT_P_THRESHOLD)
  cat("\n\nTotal significant associations across all variables:", total_sig, "\n")

} else {
  cat("\nNo association test results available for plotting\n")
}
```

## Biomarker Loadings Driving PC Extremes

```{r loadings-extremes, fig.width=14, fig.height=10}
# Identify and visualize the biomarkers with the strongest positive and negative loadings for each PC
# Uses configuration: N_TOP_LOADINGS, N_PCS_LOADINGS

cat("\n### Top Biomarker Loadings Driving Each PC\n\n")
cat("Showing top", N_TOP_LOADINGS, "positive AND top", N_TOP_LOADINGS, "negative loadings for each PC\n\n")

for (pc_num in 1:N_PCS_LOADINGS) {
  pc_name <- paste0("PC", pc_num)

  # Get loadings for this PC
  pc_loadings <- data.frame(
    biomarker = all_loadings$biomarker,
    loading = all_loadings[[pc_name]]
  )

  # Get top positive loadings
  top_positive <- pc_loadings %>%
    filter(loading > 0) %>%
    arrange(desc(loading)) %>%
    head(N_TOP_LOADINGS) %>%
    mutate(Direction = "Positive")

  # Get top negative loadings
  top_negative <- pc_loadings %>%
    filter(loading < 0) %>%
    arrange(loading) %>%
    head(N_TOP_LOADINGS) %>%
    mutate(Direction = "Negative")

  # Combine both
  top_loadings <- rbind(top_positive, top_negative)

  # Print table
  cat("\n**", pc_name, "** (", round(variance_explained[pc_num], 2), "% variance):\n\n", sep = "")

  cat("Top", N_TOP_LOADINGS, "POSITIVE loadings:\n")
  print(knitr::kable(top_positive %>% select(biomarker, loading), digits = 4, row.names = FALSE))

  cat("\nTop", N_TOP_LOADINGS, "NEGATIVE loadings:\n")
  print(knitr::kable(top_negative %>% select(biomarker, loading), digits = 4, row.names = FALSE))
  cat("\n")

  # Create bar plot for this PC
  # Order by loading value so positive is on top, negative on bottom
  top_loadings <- top_loadings %>%
    arrange(loading) %>%
    mutate(biomarker = factor(biomarker, levels = biomarker))

  p <- ggplot(top_loadings, aes(x = biomarker, y = loading, fill = Direction)) +
    geom_bar(stat = "identity") +
    scale_fill_manual(values = c("Positive" = "#C73E1D", "Negative" = "#2E86AB"),
                      name = "Direction") +
    coord_flip() +
    labs(title = paste0(pc_name, " - Top ", N_TOP_LOADINGS, " Positive & Negative Biomarker Loadings"),
         subtitle = paste0("Variance explained: ", round(variance_explained[pc_num], 2), "%"),
         x = "Biomarker",
         y = "Loading") +
    theme_minimal() +
    theme(plot.title = element_text(hjust = 0.5, face = "bold", size = 12),
          plot.subtitle = element_text(hjust = 0.5, size = 10),
          legend.position = "bottom",
          axis.text.y = element_text(size = 9))

  print(p)
}

cat("\n### Interpretation:\n")
cat("- **Positive loadings**: High values of these biomarkers push samples toward positive PC scores\n")
cat("- **Negative loadings**: High values of these biomarkers push samples toward negative PC scores\n")
cat("- **Magnitude**: Larger absolute values indicate stronger contribution to the PC\n")
cat("- Biomarkers at opposite ends of a PC often represent contrasting biological states or processes\n")
```

## PC Outlier Detection

```{r pc-outliers, fig.width=12, fig.height=10}
# Identify and visualize outlier samples in PC space
# Uses multiple methods: distance-based and statistical

cat("\n### Detecting Outliers in PC Space\n\n")

# Method 1: Mahalanobis Distance
# Calculate Mahalanobis distance using first N_PCS_TO_TEST principal components
pcs_for_outlier <- pca_scores[, 1:min(N_PCS_TO_TEST, ncol(pca_scores))]

# Calculate Mahalanobis distance
pc_center <- colMeans(pcs_for_outlier)
pc_cov <- cov(pcs_for_outlier)

# Robust Mahalanobis distance calculation
mahal_dist <- mahalanobis(pcs_for_outlier, center = pc_center, cov = pc_cov)

# Chi-square cutoff (p < 0.001) for outlier detection
# Degrees of freedom = number of PCs used
df <- ncol(pcs_for_outlier)
cutoff_chisq <- qchisq(0.999, df = df)

# Identify outliers
outlier_idx_mahal <- mahal_dist > cutoff_chisq

cat("**Method 1: Mahalanobis Distance**\n")
cat("Using first", df, "PCs\n")
cat("Chi-square cutoff (p < 0.001):", round(cutoff_chisq, 2), "\n")
cat("Number of outliers detected:", sum(outlier_idx_mahal), "/", nrow(pca_scores), "\n\n")

# Method 2: Standard Deviation Threshold for Individual PCs
# Flag samples >3 SD from mean in any of the first 5 PCs
outlier_idx_sd <- rep(FALSE, nrow(pca_scores))
outlier_pc_info <- list()

for (pc_num in 1:min(5, ncol(pca_scores))) {
  pc_name <- paste0("PC", pc_num)
  pc_values <- pca_scores[[pc_name]]

  pc_mean <- mean(pc_values)
  pc_sd <- sd(pc_values)

  # Identify samples beyond 3 SD
  outliers_this_pc <- abs(pc_values - pc_mean) > 3 * pc_sd

  if (sum(outliers_this_pc) > 0) {
    outlier_idx_sd <- outlier_idx_sd | outliers_this_pc
    outlier_pc_info[[pc_name]] <- which(outliers_this_pc)

    cat("**", pc_name, "**: ", sum(outliers_this_pc), " sample(s) beyond 3 SD\n", sep = "")
  }
}

cat("\n**Method 2: Standard Deviation Threshold (>3 SD)**\n")
cat("Total outliers detected:", sum(outlier_idx_sd), "/", nrow(pca_scores), "\n\n")

# Combine both methods
outlier_idx_combined <- outlier_idx_mahal | outlier_idx_sd

cat("**Combined Methods**\n")
cat("Total unique outliers:", sum(outlier_idx_combined), "/", nrow(pca_scores), "\n\n")

# Create outlier data frame
pca_data_outliers <- pca_data %>%
  mutate(
    Mahalanobis_Distance = mahal_dist,
    Outlier_Mahal = outlier_idx_mahal,
    Outlier_SD = outlier_idx_sd,
    Outlier_Any = outlier_idx_combined
  )

# Print outlier samples with their metadata
if (sum(outlier_idx_combined) > 0) {
  cat("### Outlier Samples:\n\n")

  outlier_samples <- pca_data_outliers %>%
    filter(Outlier_Any) %>%
    select(any_of(c("SampleID")), PC1, PC2, PC3, Mahalanobis_Distance, Outlier_Mahal, Outlier_SD,
           any_of(c("CDX", "Race_Ethnicity", "sex", "Site", "Run", "Bay"))) %>%
    arrange(desc(Mahalanobis_Distance))

  print(knitr::kable(outlier_samples, digits = 3, row.names = FALSE))

  # Save outlier list
  write.csv(outlier_samples, "pca_outliers.csv", row.names = FALSE)
  cat("\n**Outlier samples saved to:** pca_outliers.csv\n\n")
}

# Visualization 1: Mahalanobis Distance Distribution
p1 <- ggplot(pca_data_outliers, aes(x = 1:nrow(pca_data_outliers), y = Mahalanobis_Distance)) +
  geom_point(aes(color = Outlier_Mahal), size = 2, alpha = 0.7) +
  geom_hline(yintercept = cutoff_chisq, linetype = "dashed", color = "#C73E1D", linewidth = 1) +
  scale_color_manual(values = c("FALSE" = "gray50", "TRUE" = "#C73E1D"),
                     labels = c("FALSE" = "Normal", "TRUE" = "Outlier"),
                     name = "") +
  labs(title = "Mahalanobis Distance - Outlier Detection",
       subtitle = paste0("Using first ", df, " PCs; Red line = Chi-square cutoff (p<0.001)"),
       x = "Sample Index",
       y = "Mahalanobis Distance") +
  theme_minimal() +
  theme(plot.title = element_text(hjust = 0.5, face = "bold"),
        plot.subtitle = element_text(hjust = 0.5, size = 10),
        legend.position = "bottom")

print(p1)

# Visualization 2: PC1 vs PC2 with outliers highlighted
p2 <- ggplot(pca_data_outliers, aes(x = PC1, y = PC2)) +
  geom_point(aes(color = Outlier_Any, size = Outlier_Any, alpha = Outlier_Any)) +
  scale_color_manual(values = c("FALSE" = "gray50", "TRUE" = "#C73E1D"),
                     labels = c("FALSE" = "Normal", "TRUE" = "Outlier"),
                     name = "") +
  scale_size_manual(values = c("FALSE" = 2, "TRUE" = 4),
                    labels = c("FALSE" = "Normal", "TRUE" = "Outlier"),
                    name = "") +
  scale_alpha_manual(values = c("FALSE" = 0.5, "TRUE" = 1),
                     labels = c("FALSE" = "Normal", "TRUE" = "Outlier"),
                     name = "") +
  labs(title = "PCA Plot with Outliers Highlighted",
       subtitle = "Outliers identified by Mahalanobis distance or SD threshold",
       x = paste0("PC1 (", round(variance_explained[1], 2), "% variance)"),
       y = paste0("PC2 (", round(variance_explained[2], 2), "% variance)")) +
  theme_minimal() +
  theme(plot.title = element_text(hjust = 0.5, face = "bold"),
        plot.subtitle = element_text(hjust = 0.5, size = 10),
        legend.position = "bottom")

print(p2)

# Visualization 3: PC3 vs PC4 with outliers highlighted
p3 <- ggplot(pca_data_outliers, aes(x = PC3, y = PC4)) +
  geom_point(aes(color = Outlier_Any, size = Outlier_Any, alpha = Outlier_Any)) +
  scale_color_manual(values = c("FALSE" = "gray50", "TRUE" = "#C73E1D"),
                     labels = c("FALSE" = "Normal", "TRUE" = "Outlier"),
                     name = "") +
  scale_size_manual(values = c("FALSE" = 2, "TRUE" = 4),
                    labels = c("FALSE" = "Normal", "TRUE" = "Outlier"),
                    name = "") +
  scale_alpha_manual(values = c("FALSE" = 0.5, "TRUE" = 1),
                     labels = c("FALSE" = "Normal", "TRUE" = "Outlier"),
                     name = "") +
  labs(title = "PCA Plot (PC3 vs PC4) with Outliers Highlighted",
       subtitle = "Outliers identified by Mahalanobis distance or SD threshold",
       x = paste0("PC3 (", round(variance_explained[3], 2), "% variance)"),
       y = paste0("PC4 (", round(variance_explained[4], 2), "% variance)")) +
  theme_minimal() +
  theme(plot.title = element_text(hjust = 0.5, face = "bold"),
        plot.subtitle = element_text(hjust = 0.5, size = 10),
        legend.position = "bottom")

print(p3)

# Optional: If metadata variables are available, show outlier distribution by groups
if ("CDX" %in% colnames(pca_data_outliers)) {
  cat("\n### Outlier Distribution by Clinical Diagnosis:\n\n")
  outlier_by_cdx <- pca_data_outliers %>%
    group_by(CDX) %>%
    summarise(
      Total = n(),
      Outliers = sum(Outlier_Any),
      Percent_Outliers = round(100 * Outliers / Total, 1)
    )
  print(knitr::kable(outlier_by_cdx, row.names = FALSE))
}

if ("Site" %in% colnames(pca_data_outliers)) {
  cat("\n### Outlier Distribution by Site:\n\n")
  outlier_by_site <- pca_data_outliers %>%
    group_by(Site) %>%
    summarise(
      Total = n(),
      Outliers = sum(Outlier_Any),
      Percent_Outliers = round(100 * Outliers / Total, 1)
    )
  print(knitr::kable(outlier_by_site, row.names = FALSE))
}

cat("\n### Interpretation:\n")
cat("- **Mahalanobis distance**: Measures multivariate distance from the center of the PC space\n")
cat("- **SD threshold**: Flags samples with extreme values (>3 SD) in individual PCs\n")
cat("- Outliers may represent:\n")
cat("  * Technical artifacts or batch effects\n")
cat("  * Biological extremes or rare phenotypes\n")
cat("  * Sample quality issues\n")
cat("  * Mislabeled samples\n")
cat("- Consider reviewing outlier samples and their metadata for potential exclusion or investigation\n")
```

# Finish

## Save Results

```{r save-results}
# Prepare PCA scores with metadata for saving (metadata first, then PCs)
pca_scores_with_metadata <- cbind(metadata_complete, pca_scores)

# Save PCA results
write.csv(pca_scores_with_metadata, "pca_scores_with_metadata.csv", row.names = FALSE)
write.csv(all_loadings, "pca_loadings_all.csv", row.names = FALSE)

cat("Results saved:\n")
cat("- pca_scores_with_metadata.csv: PC scores with metadata columns for all samples\n")
cat("  (Contains", ncol(metadata_complete), "metadata columns +", ncol(pca_scores), "PC columns)\n")
cat("- pca_loadings_all.csv: Biomarker loadings for ALL principal components\n")
cat("  (Contains", nrow(all_loadings), "biomarkers x", ncol(all_loadings)-1, "PCs)\n")
```

## Notes

* In AA plus AFDC Analysis
  * There are some AA and AFDC HI in this dataset. It is not captured by the Group variable. Also there are some replicated samples.
  * HTT and TARDBP separate AFDC (TANZ and ABTH) from AA very well. 
  * PGK1, ANZA5 are really low in Tanzania.
  
* In overall sample without MCI
  * AB42 is low in UGA

## Session Info

```{r session-info}
sessionInfo()
```
